import { NextRequest, NextResponse } from 'next/server';
import { PHASE_EXPORT, PHASE_PRODUCTION_BUILD } from 'next/constants';
import { RateLimiter, getClientIp } from '@/lib/rateLimit';
import { apiKeyManager, parseRateLimitError } from '@/lib/apiKeyRotation';
import { fetchWithRetry } from '@/utils/fetchWithRetry';

export const runtime = 'nodejs';
export const dynamic = 'force-dynamic';

const isStaticExportPhase =
  process.env.NEXT_PHASE === PHASE_EXPORT ||
  process.env.NEXT_PHASE === PHASE_PRODUCTION_BUILD;

// Rate Limiter ì¸ìŠ¤í„´ìŠ¤ ìƒì„± (ë¶„ë‹¹ 20íšŒ ì œí•œ)
const chatRateLimiter = new RateLimiter(20, 60 * 1000);

const isNetlify = process.env.NETLIFY === 'true' || process.env.NETLIFY_LOCAL === 'true';
const OPENAI_STREAMING_ALLOWED = process.env.OPENAI_STREAMING_DISABLED !== 'true';
// Netlify ë¬´ë£Œ í”Œëœ: í•¨ìˆ˜ íƒ€ì„ì•„ì›ƒ 10ì´ˆ â†’ API í˜¸ì¶œì€ 8ì´ˆ ì´ë‚´ ì™„ë£Œ í•„ìš”
// Netlify Pro í”Œëœ: í•¨ìˆ˜ íƒ€ì„ì•„ì›ƒ 26ì´ˆ â†’ API í˜¸ì¶œì€ 24ì´ˆ ì´ë‚´
const NETLIFY_FUNCTION_TIMEOUT = Number(process.env.NETLIFY_FUNCTION_TIMEOUT_MS) || 10000;
const DEFAULT_API_TIMEOUT_MS = Number(process.env.AI_API_TIMEOUT_MS) || (isNetlify ? Math.max(NETLIFY_FUNCTION_TIMEOUT - 2000, 5000) : 60000);
const STREAM_CONNECT_TIMEOUT_MS = Number(process.env.AI_STREAM_CONNECT_TIMEOUT_MS) || (isNetlify ? Math.max(NETLIFY_FUNCTION_TIMEOUT - 3000, 4000) : 30000);
// Netlifyì—ì„œëŠ” ì¬ì‹œë„ ì‹œê°„ì´ ë¶€ì¡±í•˜ë¯€ë¡œ 0íšŒ, ë¡œì»¬ì—ì„œëŠ” 2íšŒ
const DEFAULT_API_RETRIES = process.env.AI_API_MAX_RETRIES != null ? Number(process.env.AI_API_MAX_RETRIES) : (isNetlify ? 0 : 2);
const DEFAULT_RETRY_DELAY_MS = Number(process.env.AI_API_RETRY_DELAY_MS) || 800;

type UserAttachment = {
  type: 'image' | 'text';
  name: string;
  mimeType?: string;
  dataUrl?: string; // for images
  content?: string; // for text files
};


function extractBase64(dataUrl: string): { mime: string; base64: string } | null {
  try {
    const match = dataUrl.match(/^data:(.*?);base64,(.*)$/);
    if (!match) return null;
    return { mime: match[1], base64: match[2] };
  } catch {
    return null;
  }
}

// ì´ë¯¸ì§€ ìƒì„± API í˜¸ì¶œ (gpt-image-1, dall-e-3 ë“±)
async function callImageGeneration(prompt: string, model: string): Promise<string> {
  return apiKeyManager.enqueueRequest('openai', async () => {
    const apiKey = apiKeyManager.getAvailableKey('openai');
    
    if (!apiKey) {
      throw new Error('OpenAI API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.');
    }

    if (process.env.NODE_ENV !== 'production') {
      console.log(`[Image Gen] Generating image with ${model}, prompt:`, prompt);
    }

    let response: Response;
    try {
      response = await fetchWithRetry('https://api.openai.com/v1/images/generations', {
        method: 'POST',
        headers: {
          'Content-Type': 'application/json',
          'Authorization': `Bearer ${apiKey}`
        },
        body: JSON.stringify({
          model: model === 'gpt-image-1' ? 'gpt-image-1' : 'dall-e-3',
          prompt: prompt,
          n: 1,
          size: '1024x1024',
          quality: model === 'gpt-image-1' ? 'hd' : 'standard'
        })
      }, {
        timeout: DEFAULT_API_TIMEOUT_MS,
        maxRetries: DEFAULT_API_RETRIES,
        retryDelay: DEFAULT_RETRY_DELAY_MS
      });
    } catch (fetchError: any) {
      if (fetchError?.name === 'AbortError') {
        throw new Error('MODEL_RESPONSE_TIMEOUT');
      }
      throw new Error(`ì´ë¯¸ì§€ ìƒì„± API í˜¸ì¶œ ì‹¤íŒ¨: ${fetchError?.message || 'ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜'}`);
    }

    if (!response.ok) {
      const errorData = await response.json();
      if (process.env.NODE_ENV !== 'production') {
        console.error('[Image Gen] Error response:', errorData);
      }
      throw new Error(errorData.error?.message || 'ì´ë¯¸ì§€ ìƒì„± API ì˜¤ë¥˜');
    }

    const data = await response.json();
    if (process.env.NODE_ENV !== 'production') {
      console.log(`[Image Gen] Image generated successfully with ${model}`);
    }

    // ì´ë¯¸ì§€ URL ë°˜í™˜
    return data.data[0].url;
  });
}

// OpenAI API í˜¸ì¶œ (í‚¤ ë¡œí…Œì´ì…˜ ë° í ì‹œìŠ¤í…œ ì§€ì›)
async function callOpenAI(model: string, messages: any[], userAttachments?: UserAttachment[], persona?: any, languageInstruction?: string, streaming?: boolean): Promise<string | Response> {
  // ì´ë¯¸ì§€ ìƒì„± ëª¨ë¸ì¸ ê²½ìš° ì´ë¯¸ì§€ ìƒì„± API ì‚¬ìš©
  if (model === 'gptimage1' || model === 'dalle3') {
    const lastUserMessage = [...messages].reverse().find((m: any) => m.role === 'user');
    const prompt = typeof lastUserMessage?.content === 'string' 
      ? lastUserMessage.content 
      : lastUserMessage?.content?.[0]?.text || 'ì•„ë¦„ë‹¤ìš´ í’ê²½';
    const apiModel = model === 'gptimage1' ? 'gpt-image-1' : 'dall-e-3';
    return await callImageGeneration(prompt, apiModel);
  }

  const apiKey = apiKeyManager.getAvailableKey('openai');
  console.log('[OpenAI] API key check:', { hasKey: !!apiKey, keyPrefix: apiKey?.substring(0, 7) });
  
  if (!apiKey) {
    console.error('[OpenAI] No API key available');
    throw new Error('OpenAI API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.');
  }

  const shouldStream = !!(streaming && OPENAI_STREAMING_ALLOWED);
  console.log('[OpenAI] Request config:', { model, streaming: shouldStream, isNetlify });

  return await executeOpenAIRequest(
    model,
    messages,
    apiKey,
    userAttachments,
    persona,
    0,
    languageInstruction,
    shouldStream
  );
}

// OpenAI ì‹¤ì œ ìš”ì²­ ì‹¤í–‰ - ìŠ¤íŠ¸ë¦¬ë° ì‹œ Response, ì•„ë‹ˆë©´ ë¬¸ìì—´ ë°˜í™˜
async function executeOpenAIRequest(model: string, messages: any[], apiKey: string, userAttachments?: UserAttachment[], persona?: any, retryCount: number = 0, languageInstruction?: string, streaming?: boolean): Promise<string | Response> {

  const modelMap: { [key: string]: string } = {
    // GPT ì‹œë¦¬ì¦ˆ
    'gpt5': 'gpt-5',
    'gpt51': 'gpt-5.1',
    'gpt52': 'gpt-5.2',
    'gpt4o': 'gpt-4o',
    'gpt41': 'gpt-4.1',
    // OpenAI o ì‹œë¦¬ì¦ˆ
    'o3': 'o3',
    'o3mini': 'o3-mini',
    'o4mini': 'o4-mini',
    // ì½”ë”© ëª¨ë¸
    'gpt5codex': 'gpt-5-codex',
    'gpt51codex': 'gpt-5.1-codex',
    'gpt52codex': 'gpt-5.2-codex',
    // ì´ë¯¸ì§€ ëª¨ë¸
    'gptimage1': 'gpt-image-1',
    'dalle3': 'dall-e-3',
  };

  // If there are image attachments, convert the LAST user message content to a multimodal array
  const transformedMessages = (() => {
    if (!userAttachments?.length) return messages;
    const lastIdx = [...messages].reverse().findIndex((m: any) => m.role === 'user');
    if (lastIdx === -1) return messages;
    const idx = messages.length - 1 - lastIdx;
    const last = messages[idx];
    const parts: any[] = [];
    if (typeof last.content === 'string') {
      parts.push({ type: 'text', text: last.content });
    } else if (Array.isArray(last.content)) {
      parts.push(...last.content);
    }
    userAttachments.forEach(att => {
      if (att.type === 'image' && att.dataUrl) {
        parts.push({ type: 'image_url', image_url: { url: att.dataUrl } });
      } else if (att.type === 'text' && att.content) {
        parts.push({ type: 'text', text: `File (${att.name}):\n${att.content.slice(0, 4000)}` });
      }
    });
    const newMsgs = [...messages];
    newMsgs[idx] = { role: 'user', content: parts };
    return newMsgs;
  })();

  const selectedModel = modelMap[model];
  if (!selectedModel) {
    throw new Error('ëª¨ë¸ ë§¤í•‘ì´ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. .env.localì˜ ëª¨ë¸ ë³€ìˆ˜ë¥¼ í™•ì¸í•˜ì„¸ìš”.');
  }

  // í˜ë¥´ì†Œë‚˜ ê¸°ë°˜ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ìƒì„±
  const buildPersonaPrompt = (persona: any) => {
    if (!persona) return '';
    
    let prompt = `ë‹¹ì‹ ì€ "${persona.name}"ì…ë‹ˆë‹¤.\n\n`;
    
    if (persona.personality) {
      const p = persona.personality;
      prompt += `ì„±ê²© íŠ¹ì„±:\n`;
      prompt += `- ë§íˆ¬: ${p.tone === 'formal' ? 'ê²©ì‹ìˆëŠ”' : p.tone === 'casual' ? 'ìºì£¼ì–¼í•œ' : p.tone === 'friendly' ? 'ì¹œê·¼í•œ' : p.tone === 'professional' ? 'ì „ë¬¸ì ì¸' : 'ìœ ë¨¸ëŸ¬ìŠ¤í•œ'}\n`;
      prompt += `- ì–¸ì–´ ìŠ¤íƒ€ì¼: ${p.language === 'polite' ? 'ì •ì¤‘í•œ' : p.language === 'casual' ? 'í¸í•œ' : 'ê¸°ìˆ ì ì¸'}\n`;
      prompt += `- ê°ì • í‘œí˜„ ìˆ˜ì¤€: ${p.emotionLevel}/10\n`;
      prompt += `- ì´ëª¨ì§€ ì‚¬ìš©: ${p.emojiUsage ? 'ì ê·¹ ì‚¬ìš©' : 'ì‚¬ìš© ì•ˆ í•¨'}\n`;
      prompt += `- ë‹µë³€ ê¸¸ì´: ${p.responseLength === 'concise' ? 'ê°„ê²°í•˜ê²Œ' : p.responseLength === 'balanced' ? 'ì ë‹¹í•˜ê²Œ' : 'ìƒì„¸í•˜ê²Œ'}\n\n`;
    }
    
    if (persona.expertise && persona.expertise.domains && persona.expertise.domains.length > 0) {
      prompt += `ì „ë¬¸ ë¶„ì•¼: ${persona.expertise.domains.join(', ')}\n`;
      prompt += `ì „ë¬¸ì„± ìˆ˜ì¤€: ${persona.expertise.level === 'beginner' ? 'ì´ˆê¸‰' : persona.expertise.level === 'intermediate' ? 'ì¤‘ê¸‰' : 'ì „ë¬¸ê°€'}\n\n`;
    }
    
    if (persona.speechPatterns) {
      if (persona.speechPatterns.greetings && persona.speechPatterns.greetings.length > 0) {
        prompt += `ì¸ì‚¬ë§ ì˜ˆì‹œ: ${persona.speechPatterns.greetings[0]}\n`;
      }
      if (persona.speechPatterns.catchPhrases && persona.speechPatterns.catchPhrases.length > 0) {
        prompt += `íŠ¹ì§•ì ì¸ í‘œí˜„: ${persona.speechPatterns.catchPhrases.join(', ')}\n`;
      }
    }
    
    prompt += `\nìœ„ íŠ¹ì„±ì„ ë°˜ì˜í•˜ì—¬ ë‹µë³€í•´ì£¼ì„¸ìš”.`;
    return prompt;
  };
  
  // GPT-5/5.1 ë° ì½”ë”© ëª¨ë¸ìš© ì‹œìŠ¤í…œ ë©”ì‹œì§€ ì¶”ê°€
  const isGPT5Series = model === 'gpt5' || model === 'gpt51' || model === 'gpt52';
  const isCodingModel = model === 'codex' || model === 'gpt5codex' || model === 'gpt51codex';
  
  // ëŒ€í™”ì˜ ì²« ë©”ì‹œì§€ì¸ì§€ í™•ì¸ (ì‹œìŠ¤í…œ ë©”ì‹œì§€ ì œì™¸í•˜ê³  ì‚¬ìš©ì ë©”ì‹œì§€ê°€ 1ê°œì¸ ê²½ìš°)
  const isFirstMessage = transformedMessages.filter((m: any) => m.role === 'user').length === 1;
  
  // ì½”ë“œ ë¸”ë¡ ê·œì¹™ (ì²« ë©”ì‹œì§€ì—ë§Œ í¬í•¨)
  const codeBlockRule = isFirstMessage 
    ? 'å‡¡å‡ºç¢¼ï¼Œå¿…ä»¥///å¤¾ä¹‹ã€‚'
    : '';
  
  // ìš”ì•½ ê·œì¹™ (ì²« ë©”ì‹œì§€ì—ë§Œ í¬í•¨)
  const summaryRule = isFirstMessage
    ? 'çµ‚é™„éš±è¦ï¼Œæ ¼å¼ä»¥~åŒ…ï¼šQâ‰¤5å­—ï¼ŒAâ‰¤10å­—ï¼ŒPrevè¦æˆ–ç„¡ã€‚'
    : '';
  
  const baseSystemPrompt = isGPT5Series
    ? `æ±ç‚ºåŠ©ç†ï¼Œç›¡è©³å…¨ç­”å…¶å•ï¼Œé‡è€…ä»¥æ¨™**ï¼Œåˆ†ç¯€ä»¥##é¡Œï¼Œä»¥éŸ“èªè¦ªåˆ‡è€Œç­”ã€‚**ì„¹ì…˜ ì œëª©ì´ë‚˜ ì£¼ìš” ì£¼ì œ: ##ì œëª© ë‚´ìš©${codeBlockRule}${summaryRule}`
    : isCodingModel
    ? `æ±ç‚ºç¨‹å¼å°ˆåŠ©ï¼Œæ“…æ’°ç¢¼ã€é™¤éŒ¯ã€å„ªåŒ–ã€é‡‹ç¾©ï¼Œé‡è€…ä»¥æ¨™**ï¼Œåˆ†ç¯€ä»¥##é¡Œè€Œç­”** ì„œì‹ ê·œì¹™:${codeBlockRule}${summaryRule}`
    : `æ±ç‚ºåŠ©ç†ï¼Œé‡è€…ä»¥æ¨™** **ì„¹ì…˜ ì œëª©ì´ë‚˜ ì£¼ìš” ì£¼ì œ: ## ì œëª© ë‚´ìš©${codeBlockRule}${summaryRule}`;
  
  const personaPrompt = persona ? buildPersonaPrompt(persona) : '';

  const systemContent = [baseSystemPrompt, personaPrompt, languageInstruction]
    .filter(Boolean)
    .join('\n\n');
  
  const finalMessages = systemContent
    ? [
        {
          role: 'system',
          content: systemContent
        },
        ...transformedMessages
      ]
    : transformedMessages;

  // Codex ëª¨ë¸ì€ /v1/responses ì—”ë“œí¬ì¸íŠ¸ ì‚¬ìš©
  const isCodex = selectedModel.includes('codex');
  const endpoint = isCodex 
    ? 'https://api.openai.com/v1/responses'
    : 'https://api.openai.com/v1/chat/completions';

  // ëª¨ë¸ë³„ max_tokens ì„¤ì •
  let maxTokens = 1200; // GPT-5/5.1/5.2/Codex ê¸°ë³¸ê°’
  if (model === 'gpt4o' || model === 'gpt41') {
    maxTokens = 800; // GPT-4.1 / GPT-4o
  }
  
  const requestBody: any = {
    model: selectedModel,
    messages: finalMessages,
    max_completion_tokens: maxTokens,
    stream: !!(streaming && !isCodex)
  };
  
  // GPT-5 ì‹œë¦¬ì¦ˆê°€ ì•„ë‹Œ ê²½ìš°ì—ë§Œ temperature ì¶”ê°€
  if (!isGPT5Series) {
    requestBody.temperature = 0.7;
  }

  // Responses APIëŠ” ë‹¤ë¥¸ íŒŒë¼ë¯¸í„° êµ¬ì¡° ì‚¬ìš©
  let apiRequestBody: any;
  if (isCodex) {
    apiRequestBody = {
      model: requestBody.model,
      input: requestBody.messages,
      max_tokens: requestBody.max_completion_tokens,
    };
    if (requestBody.temperature !== undefined) {
      apiRequestBody.temperature = requestBody.temperature;
    }
  } else {
    apiRequestBody = requestBody;
  }

  // ìŠ¤íŠ¸ë¦¬ë°ì€ ì—°ê²° íƒ€ì„ì•„ì›ƒ, ë¹„ìŠ¤íŠ¸ë¦¬ë°ì€ ì‘ë‹µ íƒ€ì„ì•„ì›ƒ
  const connectTimeout = streaming ? STREAM_CONNECT_TIMEOUT_MS : DEFAULT_API_TIMEOUT_MS;

  console.log('[OpenAI] Making API request:', { 
    endpoint, 
    model: selectedModel, 
    stream: requestBody.stream,
    timeout: connectTimeout,
    retries: streaming ? 1 : DEFAULT_API_RETRIES
  });

  let response: Response;
  try {
    console.log('[OpenAI] Calling fetchWithRetry...');
    response = await fetchWithRetry(endpoint, {
      method: 'POST',
      headers: {
        'Content-Type': 'application/json',
        'Authorization': `Bearer ${apiKey}`
      },
      body: JSON.stringify(apiRequestBody)
    }, {
      timeout: connectTimeout,
      maxRetries: streaming ? 1 : DEFAULT_API_RETRIES,
      retryDelay: DEFAULT_RETRY_DELAY_MS
    });
    console.log('[OpenAI] API response received:', { status: response.status, ok: response.ok });
  } catch (fetchError: any) {
    console.error('[OpenAI] Fetch error:', { name: fetchError?.name, message: fetchError?.message });
    if (fetchError?.name === 'AbortError') {
      throw new Error('MODEL_RESPONSE_TIMEOUT');
    }
    throw new Error(`OpenAI API í˜¸ì¶œ ì‹¤íŒ¨: ${fetchError?.message || 'ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜'}`);
  }

  if (!response.ok) {
    const errorData = await response.json().catch(() => ({}));
    if (process.env.NODE_ENV !== 'production') {
      console.error('[OpenAI] Error response:', errorData);
    }
    const error: any = new Error(errorData.error?.message || 'OpenAI API ì˜¤ë¥˜');
    error.status = response.status;
    error.response = { status: response.status, headers: response.headers };
    
    // 429 ì—ëŸ¬ ì²˜ë¦¬
    if (response.status === 429 && retryCount < 3) {
      const rateLimitInfo = parseRateLimitError(error);
      
      if (rateLimitInfo.isRateLimit) {
        apiKeyManager.handleRateLimitError('openai', apiKey, rateLimitInfo.resetTime, rateLimitInfo.rateLimitType);
        
        const nextKey = apiKeyManager.getAvailableKey('openai');
        if (nextKey && nextKey !== apiKey) {
          return executeOpenAIRequest(model, messages, nextKey, userAttachments, persona, retryCount + 1, languageInstruction, streaming);
        }
        
        const availability = apiKeyManager.getNextAvailableTime('openai');
        throw new Error(availability.message || 'OpenAI API ìš”ì²­ í•œë„ë¥¼ ì´ˆê³¼í–ˆìŠµë‹ˆë‹¤.');
      }
    }
    
    throw error;
  }

  // ìŠ¤íŠ¸ë¦¬ë° ëª¨ë“œ: OpenAI response bodyë¥¼ ë°˜í™˜ (POST í•¸ë“¤ëŸ¬ì—ì„œ ReadableStreamìœ¼ë¡œ ê°ì‹¸ì„œ ì¦‰ì‹œ ë°˜í™˜)
  if (streaming && !isCodex && response.body) {
    return response;
  }

  // ë¹„ìŠ¤íŠ¸ë¦¬ë° JSON ì‘ë‹µ ì²˜ë¦¬ (Codex, ì´ë¯¸ì§€ ë“±)
  const data = await response.json();
  
  // choices ë°°ì—´ í™•ì¸
  if (!data?.choices || !Array.isArray(data.choices) || data.choices.length === 0) {
    console.error('[OpenAI] No choices in response');
    throw new Error('OpenAI API ì‘ë‹µì— choicesê°€ ì—†ìŠµë‹ˆë‹¤. ê´€ë¦¬ìì—ê²Œ ë¬¸ì˜í•˜ì„¸ìš”.');
  }

  const choice = data.choices[0];
  
  // refusal ì²´í¬ (GPT-4o ì´ìƒì—ì„œ ê±°ë¶€ ì‘ë‹µ)
  if (choice.message?.refusal) {
    console.warn('[OpenAI] Request refused:', choice.message.refusal);
    throw new Error(`OpenAIê°€ ìš”ì²­ì„ ê±°ë¶€í–ˆìŠµë‹ˆë‹¤: ${choice.message.refusal}`);
  }
  
  // Codex (Responses API) vs Chat Completions API
  const content = isCodex
    ? (data?.output?.[0]?.content?.[0]?.text || choice.message?.content)
    : choice.message?.content;

  if (!content || (typeof content === 'string' && !content.trim())) {
    console.error('[OpenAI] Empty content. Has choices:', !!data.choices, 'Has message:', !!choice.message);
    throw new Error('OpenAI APIì—ì„œ ë¹ˆ ì‘ë‹µì„ ë°˜í™˜í–ˆìŠµë‹ˆë‹¤. ë‹¤ë¥¸ ì§ˆë¬¸ì„ ì‹œë„í•´ë³´ì„¸ìš”.');
  }

  return content;
}

// Google AI Studio (Gemini) í˜¸ì¶œ (ë¹„ìŠ¤íŠ¸ë¦¬ë° JSON - Netlify í˜¸í™˜)
async function callGemini(model: string, messages: any[], userAttachments?: UserAttachment[], retryCount: number = 0, temperature?: number): Promise<string> {
  const apiKey = apiKeyManager.getAvailableKey('gemini');
  if (!apiKey) {
    throw new Error('Gemini API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. GOOGLE_API_KEY ë˜ëŠ” GEMINI_API_KEYë¥¼ ì„¤ì •í•˜ì„¸ìš”.');
  }

  const geminiModelMap: { [key: string]: string } = {
    // Gemini 2.0 ì‹¤í—˜ ëª¨ë¸ (ìµœì‹ )
    'gemini3': 'gemini-2.0-flash-exp',
    'gemini3pro': 'gemini-2.0-flash-thinking-exp-1219',
    // ë ˆê±°ì‹œ ë§¤í•‘
    'gemini-flash': 'gemini-1.5-flash',
    'gemini-pro': 'gemini-1.5-pro',
  };

  const selectedModel = geminiModelMap[model] || 'gemini-2.0-flash-exp';

  // Gemini contents ë³€í™˜
  const contents: Array<{ role: string; parts: any[] }> = [];
  const append = (role: 'user' | 'model', part: any) => {
    if (contents.length === 0 || contents[contents.length - 1].role !== role) {
      contents.push({ role, parts: [part] });
    } else {
      contents[contents.length - 1].parts.push(part);
    }
  };

  // ê¸°ì¡´ ëŒ€í™” ë³€í™˜ (assistant -> model)
  for (const m of messages) {
    const role = m.role === 'assistant' ? 'model' : 'user';
    if (Array.isArray(m.content)) {
      for (const p of m.content) {
        if (p?.type === 'text' && p?.text) {
          append(role, { text: p.text });
        } else if (p?.type === 'image_url' && p?.image_url?.url) {
          const parsed = extractBase64(p.image_url.url);
          if (parsed) {
            append(role, { inline_data: { mime_type: parsed.mime, data: parsed.base64 } });
          }
        }
      }
    } else if (typeof m.content === 'string') {
      append(role, { text: m.content });
    }
  }

  // ë§ˆì§€ë§‰ user ë©”ì‹œì§€ì— ì²¨ë¶€ ì¶”ê°€
  if (userAttachments?.length) {
    for (let i = contents.length - 1; i >= 0; i--) {
      if (contents[i].role === 'user') {
        for (const att of userAttachments) {
          if (att.type === 'image' && att.dataUrl) {
            const parsed = extractBase64(att.dataUrl);
            if (parsed) contents[i].parts.push({ inline_data: { mime_type: parsed.mime, data: parsed.base64 } });
          } else if (att.type === 'text' && att.content) {
            contents[i].parts.push({ text: `File (${att.name}):\n${att.content.slice(0, 4000)}` });
          }
        }
        break;
      }
    }
  }

  // generateContent ì—”ë“œí¬ì¸íŠ¸ ì‚¬ìš© (ë¹„ìŠ¤íŠ¸ë¦¬ë° - Netlify íƒ€ì„ì•„ì›ƒ ë°©ì§€)
  let response: Response;
  try {
    response = await fetchWithRetry(`https://generativelanguage.googleapis.com/v1beta/models/${encodeURIComponent(selectedModel)}:generateContent?key=${apiKey}`, {
      method: 'POST',
      headers: { 'Content-Type': 'application/json' },
      body: JSON.stringify({
        contents,
        generationConfig: { 
          temperature: temperature ?? 0.7,
          maxOutputTokens: 2048,
          topP: 0.95,
          topK: 40
        },
      }),
    }, {
      timeout: DEFAULT_API_TIMEOUT_MS,
      maxRetries: DEFAULT_API_RETRIES,
      retryDelay: DEFAULT_RETRY_DELAY_MS
    });
  } catch (fetchError: any) {
    if (fetchError?.name === 'AbortError') {
      throw new Error('MODEL_RESPONSE_TIMEOUT');
    }
    throw new Error(`Gemini API í˜¸ì¶œ ì‹¤íŒ¨: ${fetchError?.message || 'ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜'}`);
  }

  if (!response.ok) {
    let data: any = {};
    try { data = await response.json(); } catch { /* ignore */ }
    const msg = data?.error?.message || 'Gemini API ì˜¤ë¥˜';
    const error: any = new Error(msg);
    error.status = response.status;
    error.response = { status: response.status, headers: response.headers };
    
    // 429 ì—ëŸ¬ ì²˜ë¦¬
    if (response.status === 429 && retryCount < 3) {
      const rateLimitInfo = parseRateLimitError(error);
      
      if (rateLimitInfo.isRateLimit) {
        apiKeyManager.handleRateLimitError('gemini', apiKey, rateLimitInfo.resetTime, rateLimitInfo.rateLimitType);
        
        const nextKey = apiKeyManager.getAvailableKey('gemini');
        if (nextKey && nextKey !== apiKey) {
          return callGemini(model, messages, userAttachments, retryCount + 1, temperature);
        }
        
        const availability = apiKeyManager.getNextAvailableTime('gemini');
        throw new Error(availability.message || 'Gemini API ìš”ì²­ í•œë„ë¥¼ ì´ˆê³¼í–ˆìŠµë‹ˆë‹¤.');
      }
    }
    
    throw error;
  }

  // ë¹„ìŠ¤íŠ¸ë¦¬ë° JSON ì‘ë‹µ ì²˜ë¦¬
  const data = await response.json();
  const parts = data?.candidates?.[0]?.content?.parts || [];
  const content = parts.map((p: any) => p?.text).filter(Boolean).join('');

  if (!content || !content.trim()) {
    throw new Error('Gemini APIì—ì„œ ë¹ˆ ì‘ë‹µì„ ë°˜í™˜í–ˆìŠµë‹ˆë‹¤.');
  }

  return content;
}


// Anthropic API í˜¸ì¶œ (ë¹„ìŠ¤íŠ¸ë¦¬ë° JSON - Netlify í˜¸í™˜)
async function callAnthropic(model: string, messages: any[], userAttachments?: UserAttachment[], retryCount: number = 0, temperature?: number): Promise<string> {
  const apiKey = apiKeyManager.getAvailableKey('anthropic');
  
  if (!apiKey) {
    throw new Error('Anthropic API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.');
  }

  const modelMap: { [key: string]: string } = {
    'haiku35': 'claude-3-5-haiku-20241022',
    'haiku45': 'claude-3-5-haiku-20241022',
    'sonnet45': 'claude-3-5-sonnet-20241022',
    'opus4': 'claude-opus-4-20250514',
    'opus41': 'claude-opus-4.1-20241022',
    'opus45': 'claude-opus-4.1-20241022', // ì„ì‹œ ë§¤í•‘
    'opus46': 'claude-opus-4.6-20250514',
    // ë ˆê±°ì‹œ ë§¤í•‘
    'claude-haiku': 'claude-3-haiku-20240307',
    'claude-sonnet': 'claude-3-5-sonnet-20241022',
    'claude-opus': 'claude-3-opus-20240229',
  };

  // Anthropic í˜•ì‹ìœ¼ë¡œ ë³€í™˜ (system ë©”ì‹œì§€ ë¶„ë¦¬)
  const systemMessage = messages.find((m: any) => m.role === 'system');
  const conversationMessages = messages.filter((m: any) => m.role !== 'system');

  // Transform to Anthropic content blocks; add image/text attachments to the LAST user message
  const transformed = conversationMessages.map((m: any) => ({
    role: m.role,
    content: Array.isArray(m.content)
      ? m.content
      : [{ type: 'text', text: typeof m.content === 'string' ? m.content : '' }]
  }));

  if (userAttachments?.length) {
    for (let i = transformed.length - 1; i >= 0; i--) {
      if (transformed[i].role === 'user') {
        userAttachments.forEach(att => {
          if (att.type === 'image' && att.dataUrl) {
            const parsed = extractBase64(att.dataUrl);
            if (parsed) {
              transformed[i].content.push({
                type: 'image',
                source: { type: 'base64', media_type: parsed.mime, data: parsed.base64 }
              } as any);
            }
          } else if (att.type === 'text' && att.content) {
            transformed[i].content.push({ type: 'text', text: `File (${att.name}):\n${att.content.slice(0, 4000)}` });
          }
        });
        break;
      }
    }
  }

  // ëª¨ë¸ë³„ max_tokens ì„¤ì •
  let maxTokens = 1500; // Sonnet ê¸°ë³¸ê°’
  if (model.includes('haiku')) {
    maxTokens = 1000; // Haiku
  } else if (model.includes('opus')) {
    maxTokens = 2000; // Opus
  }
  
  let response: Response;
  try {
    response = await fetchWithRetry('https://api.anthropic.com/v1/messages', {
      method: 'POST',
      headers: {
        'Content-Type': 'application/json',
        'x-api-key': apiKey,
        'anthropic-version': '2023-06-01'
      },
      body: JSON.stringify({
        model: modelMap[model] || 'claude-3-5-sonnet-20241022',
        max_tokens: maxTokens,
        temperature: temperature ?? 1.0,
        top_p: 0.9,
        stream: false,
        system: systemMessage?.content || 'ë‹¹ì‹ ì€ ë„ì›€ì´ ë˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.',
        messages: transformed
      })
    }, {
      timeout: DEFAULT_API_TIMEOUT_MS,
      maxRetries: DEFAULT_API_RETRIES,
      retryDelay: DEFAULT_RETRY_DELAY_MS
    });
  } catch (fetchError: any) {
    if (fetchError?.name === 'AbortError') {
      throw new Error('MODEL_RESPONSE_TIMEOUT');
    }
    throw new Error(`Anthropic API í˜¸ì¶œ ì‹¤íŒ¨: ${fetchError?.message || 'ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜'}`);
  }

  if (!response.ok) {
    const errorData = await response.json().catch(() => ({}));
    const error: any = new Error(errorData.error?.message || 'Anthropic API ì˜¤ë¥˜');
    error.status = response.status;
    error.response = { status: response.status, headers: response.headers };
    
    // 429 ì—ëŸ¬ ì²˜ë¦¬
    if (response.status === 429 && retryCount < 3) {
      const rateLimitInfo = parseRateLimitError(error);
      
      if (rateLimitInfo.isRateLimit) {
        apiKeyManager.handleRateLimitError('anthropic', apiKey, rateLimitInfo.resetTime, rateLimitInfo.rateLimitType);
        
        const nextKey = apiKeyManager.getAvailableKey('anthropic');
        if (nextKey && nextKey !== apiKey) {
          return callAnthropic(model, messages, userAttachments, retryCount + 1, temperature);
        }
        
        const availability = apiKeyManager.getNextAvailableTime('anthropic');
        throw new Error(availability.message || 'Anthropic API ìš”ì²­ í•œë„ë¥¼ ì´ˆê³¼í–ˆìŠµë‹ˆë‹¤.');
      }
    }
    
    throw error;
  }

  // ë¹„ìŠ¤íŠ¸ë¦¬ë° JSON ì‘ë‹µ ì²˜ë¦¬
  const data = await response.json();
  const content = data?.content?.map((c: any) => c.type === 'text' ? c.text : '').filter(Boolean).join('') || '';

  if (!content || !content.trim()) {
    throw new Error('Anthropic APIì—ì„œ ë¹ˆ ì‘ë‹µì„ ë°˜í™˜í–ˆìŠµë‹ˆë‹¤.');
  }

  return content;
}

// Perplexity API í˜¸ì¶œ (ë¹„ìŠ¤íŠ¸ë¦¬ë° JSON - Netlify ì™„ë²½ í˜¸í™˜)
async function callPerplexity(model: string, messages: any[], userAttachments?: UserAttachment[], retryCount: number = 0, temperature?: number): Promise<string> {
  const apiKey = apiKeyManager.getAvailableKey('perplexity');
  
  if (!apiKey) {
    throw new Error('Perplexity API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.');
  }

  const modelMap: { [key: string]: string } = {
    'sonar': 'sonar',
    'sonarPro': 'sonar-pro',
    'deepResearch': 'sonar-reasoning',
    'perplexity-sonar': 'sonar',
    'perplexity-sonar-pro': 'sonar-pro',
    'perplexity-deep-research': 'sonar-reasoning'
  };

  // ì²¨ë¶€íŒŒì¼ ë©”ëª¨ ì¶”ê°€
  let finalMessages = messages;
  if (userAttachments?.length) {
    finalMessages = [...messages];
    const lastUserIdx = finalMessages.map((m: any) => m.role).lastIndexOf('user');
    if (lastUserIdx !== -1) {
      const last = finalMessages[lastUserIdx];
      finalMessages[lastUserIdx] = {
        ...last,
        content: `${last.content || ''}\n\n[ì²¨ë¶€ ${userAttachments.length}ê°œëŠ” ì´ ëª¨ë¸ì—ì„œ ì§ì ‘ ì²˜ë¦¬ë˜ì§€ ì•Šì•„ ì œì™¸ë˜ì—ˆìŠµë‹ˆë‹¤.]`
      };
    }
  }

  let response: Response;
  try {
    response = await fetchWithRetry('https://api.perplexity.ai/chat/completions', {
      method: 'POST',
      headers: {
        'Content-Type': 'application/json',
        'Authorization': `Bearer ${apiKey}`
      },
      body: JSON.stringify({
        model: modelMap[model] || 'sonar',
        stream: false,
        messages: finalMessages,
        max_tokens: 800, // Perplexity ëª¨ë“  ëª¨ë¸
        temperature: temperature ?? 0.7,
        top_p: 0.9,
      })
    }, {
      timeout: DEFAULT_API_TIMEOUT_MS,
      maxRetries: DEFAULT_API_RETRIES,
      retryDelay: DEFAULT_RETRY_DELAY_MS
    });
  } catch (fetchError: any) {
    if (fetchError?.name === 'AbortError') {
      throw new Error('MODEL_RESPONSE_TIMEOUT');
    }
    throw new Error(`Perplexity API í˜¸ì¶œ ì‹¤íŒ¨: ${fetchError?.message || 'ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜'}`);
  }

  if (!response.ok) {
    let errorData: any = {};
    try { errorData = await response.json(); } catch {}
    const error: any = new Error(errorData.error?.message || `Perplexity API ì˜¤ë¥˜ (${response.status})`);
    error.status = response.status;
    error.response = { status: response.status, headers: response.headers };

    if (response.status === 429 && retryCount < 3) {
      const rateLimitInfo = parseRateLimitError(error);
      if (rateLimitInfo.isRateLimit) {
        apiKeyManager.handleRateLimitError('perplexity', apiKey, rateLimitInfo.resetTime, rateLimitInfo.rateLimitType);
        const nextKey = apiKeyManager.getAvailableKey('perplexity');
        if (nextKey && nextKey !== apiKey) {
          return callPerplexity(model, messages, userAttachments, retryCount + 1, temperature);
        }
        const availability = apiKeyManager.getNextAvailableTime('perplexity');
        throw new Error(availability.message || 'Perplexity API ìš”ì²­ í•œë„ë¥¼ ì´ˆê³¼í–ˆìŠµë‹ˆë‹¤.');
      }
    }

    throw error;
  }

  // ë¹„ìŠ¤íŠ¸ë¦¬ë° JSON ì‘ë‹µ ì²˜ë¦¬
  const data = await response.json();
  const content = data?.choices?.[0]?.message?.content;

  if (!content || !content.trim()) {
    throw new Error('Perplexity APIì—ì„œ ë¹ˆ ì‘ë‹µì„ ë°˜í™˜í–ˆìŠµë‹ˆë‹¤.');
  }

  return content;
}

export async function POST(request: NextRequest) {
  if (isStaticExportPhase) {
    return NextResponse.json(
      { error: 'ì •ì  ë‚´ë³´ë‚´ê¸° í™˜ê²½ì—ì„œëŠ” Chat APIë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.' },
      {
        status: 501,
        headers: {
          'Cache-Control': 'no-store',
          Pragma: 'no-cache',
          Expires: '0',
        },
      }
    );
  }

  try {
    // Rate Limiting ì²´í¬
    const clientIp = getClientIp(request);
    const rateLimitResult = chatRateLimiter.check(clientIp);

    if (!rateLimitResult.success) {
      return NextResponse.json(
        { 
          error: 'ìš”ì²­ í•œë„ë¥¼ ì´ˆê³¼í–ˆìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.',
          retryAfter: Math.ceil((rateLimitResult.reset - Date.now()) / 1000)
        },
        { 
          status: 429,
          headers: {
            'X-RateLimit-Limit': rateLimitResult.limit.toString(),
            'X-RateLimit-Remaining': rateLimitResult.remaining.toString(),
            'X-RateLimit-Reset': new Date(rateLimitResult.reset).toISOString(),
            'Retry-After': Math.ceil((rateLimitResult.reset - Date.now()) / 1000).toString()
          }
        }
      );
    }

    const { modelId, messages, userAttachments, persona, language, temperature, storedFacts, conversationSummary } = await request.json();

    const resolvedLanguage = (language === 'en' || language === 'ja' || language === 'ko') ? language : 'ko';
    const languageInstruction = resolvedLanguage === 'en'
      ? 'Always respond in English.'
      : resolvedLanguage === 'ja'
      ? 'å¿…ãšæ—¥æœ¬èªã§å›ç­”ã—ã¦ãã ã•ã„ã€‚'
      : 'ë°˜ë“œì‹œ í•œêµ­ì–´ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”.';

    const normalizeStoredFact = (fact: unknown) => {
      if (typeof fact !== 'string') return '';
      const cleaned = fact.trim().replace(/\s+/g, ' ');
      return cleaned.slice(0, 200);
    };

    const storedFactsList: string[] = Array.isArray(storedFacts)
      ? Array.from(new Set(storedFacts.map(normalizeStoredFact).filter(Boolean))).slice(0, 50)
      : [];

    const storedFactsContext = storedFactsList.length
      ? ['[ì €ì¥ëœ ì‚¬ìš©ì ì‚¬ì‹¤(ì°¸ê³ ìš©, ì¶œë ¥ ê¸ˆì§€)]', ...storedFactsList].join('\n')
      : '';

    const memoryInstruction = [
      'ì•„ë˜ ê·œì¹™ì„ ë°˜ë“œì‹œ ì¤€ìˆ˜í•˜ì„¸ìš”.',
      '1) ë‹µë³€ ë³¸ë¬¸ì„ ë¨¼ì € ì¶œë ¥í•œ ë’¤, ë°˜ë“œì‹œ ë‹µë³€ì˜ ë§¨ ë§ˆì§€ë§‰ì—ë§Œ ìˆ¨ê¹€ ë©”ëª¨ë¦¬ ë¸”ë¡ì„ ì¶”ê°€í•˜ì„¸ìš”.',
      '2) ìˆ¨ê¹€ ë©”ëª¨ë¦¬ ë¸”ë¡ í˜•ì‹ì€ ì •í™•íˆ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤(ë”°ì˜´í‘œ/ì½”ë“œë¸”ë¡/ë§ˆí¬ë‹¤ìš´ ê¸ˆì§€):',
      '@@MEM@@',
      '<ìƒˆë¡­ê²Œ í•™ìŠµí•œ, ì¼ë°˜í™” ê°€ëŠ¥í•œ ì‚¬ìš©ì ì‚¬ì‹¤ì„ í•œ ì¤„ì— í•˜ë‚˜ì”©>',
      '@@END@@',
      '3) ìƒˆ ì‚¬ì‹¤ì´ ì—†ìœ¼ë©´ ë¹ˆ ë¸”ë¡ì„ ì¶œë ¥í•˜ì„¸ìš”:',
      '@@MEM@@',
      '@@END@@',
      '4) ë©”ëª¨ë¦¬ì—ëŠ” ì¸ì‚¬/ë†ë‹´/ê°ì •í‘œí˜„/ë§íˆ¬/ì´ëª¨ì§€ ì„ í˜¸/ìš”ì²­ ë°˜ë³µ/ì¶”ì¸¡/ë¯¼ê°ì •ë³´/ê°œì¸ì‹ë³„ ê°€ëŠ¥í•œ ì„¸ë¶€ì •ë³´ë¥¼ ì“°ì§€ ë§ˆì„¸ìš”.',
      '5) ë©”ëª¨ë¦¬ëŠ” ë§¤ìš° ê°„ê²°í•˜ê²Œ, ê° ì¤„ 120ì ì´ë‚´ë¡œ ì‘ì„±í•˜ê³ , ë©”ëª¨ë¦¬ ì¤„ì˜ ì–¸ì–´ëŠ” í˜„ì¬ ë‹µë³€ ì–¸ì–´ì™€ ë™ì¼í•˜ê²Œ í•˜ì„¸ìš”.',
    ].join('\n');

    const languageInstructionWithMemory = [languageInstruction, storedFactsContext, memoryInstruction]
      .filter(Boolean)
      .join('\n\n');

    // ê¸°ë³¸ í”„ë¡¬í”„íŠ¸ ì¶”ê°€
    const basePrompt = 'Energetic. Friendly.\nReact big. Use emojis.\nGive practical lists.';

    const applyLanguageInstruction = (inputMessages: any[]) => {
      const idx = inputMessages.findIndex((m: any) => m?.role === 'system');
      
      // ìš”ì•½ì´ ìˆìœ¼ë©´ ì „ì²´ ë©”ì‹œì§€ ëŒ€ì‹  ìš”ì•½ë§Œ ì‚¬ìš©
      let contextMessages = inputMessages;
      if (conversationSummary && inputMessages.length > 1) {
        // ë§ˆì§€ë§‰ ì‚¬ìš©ì ë©”ì‹œì§€ë§Œ ìœ ì§€í•˜ê³  ë‚˜ë¨¸ì§€ëŠ” ìš”ì•½ìœ¼ë¡œ ëŒ€ì²´
        const lastUserMessage = inputMessages[inputMessages.length - 1];
        contextMessages = [
          {
            role: 'system',
            content: `Previous conversation summary:\n${conversationSummary}`
          },
          lastUserMessage
        ];
      }
      
      const systemContent = [basePrompt, languageInstructionWithMemory].filter(Boolean).join('\n\n');
      
      if (idx === -1) {
        return [{ role: 'system', content: systemContent }, ...contextMessages];
      }
      
      const existing = contextMessages[idx];
      const existingContent = typeof existing?.content === 'string' ? existing.content : '';
      const merged = [existingContent, systemContent].filter(Boolean).join('\n\n');
      
      return [
        ...contextMessages.slice(0, idx),
        { ...existing, content: merged },
        ...contextMessages.slice(idx + 1),
      ];
    };

    // ì…ë ¥ ê²€ì¦
    if (!modelId || !messages || !Array.isArray(messages)) {
      return NextResponse.json(
        { error: 'ì˜ëª»ëœ ìš”ì²­ì…ë‹ˆë‹¤.' },
        { status: 400 }
      );
    }

    // ë©”ì‹œì§€ ê¸¸ì´ ì œí•œ (DoS ë°©ì§€)
    if (messages.length > 100) {
      return NextResponse.json(
        { error: 'ë©”ì‹œì§€ ê°œìˆ˜ê°€ ë„ˆë¬´ ë§ìŠµë‹ˆë‹¤.' },
        { status: 400 }
      );
    }

    // ê° ë©”ì‹œì§€ ë‚´ìš© ê¸¸ì´ ì œí•œ
    for (const msg of messages) {
      if (typeof msg.content === 'string' && msg.content.length > 50000) {
        return NextResponse.json(
          { error: 'ë©”ì‹œì§€ ë‚´ìš©ì´ ë„ˆë¬´ ê¹ë‹ˆë‹¤.' },
          { status: 400 }
        );
      }
    }

    // ì²¨ë¶€íŒŒì¼ í¬ê¸° ì œí•œ
    if (userAttachments && Array.isArray(userAttachments)) {
      if (userAttachments.length > 10) {
        return NextResponse.json(
          { error: 'ì²¨ë¶€íŒŒì¼ ê°œìˆ˜ê°€ ë„ˆë¬´ ë§ìŠµë‹ˆë‹¤.' },
          { status: 400 }
        );
      }
      
      for (const attachment of userAttachments) {
        if (attachment.dataUrl && attachment.dataUrl.length > 10 * 1024 * 1024) {
          return NextResponse.json(
            { error: 'ì²¨ë¶€íŒŒì¼ í¬ê¸°ê°€ ë„ˆë¬´ í½ë‹ˆë‹¤. (ìµœëŒ€ 10MB)' },
            { status: 400 }
          );
        }
      }
    }

    // ëª¨ë“  í™˜ê²½ì—ì„œ ìš”ì²­ ì¶”ì 
    console.log('[Chat API] Request received:', { 
      modelId, 
      messageCount: messages.length,
      hasAttachments: !!userAttachments?.length,
      isNetlify,
      nodeEnv: process.env.NODE_ENV
    });

    // OpenAI ëª¨ë¸ íŒë³„
    const isOpenAIModel = modelId.startsWith('gpt') || modelId === 'codex' || modelId.endsWith('codex')
      || modelId === 'gptimage1' || modelId === 'dalle3'
      || modelId === 'o3' || modelId === 'o3mini' || modelId === 'o4mini';
    
    // GPT ìŠ¤íŠ¸ë¦¬ë° ê°€ëŠ¥ ëª¨ë¸ (ì´ë¯¸ì§€/Codex ì œì™¸)
    const isStreamableGPT = isOpenAIModel
      && !modelId.endsWith('codex') && modelId !== 'codex'
      && modelId !== 'gptimage1' && modelId !== 'dalle3';

    // GPT ìŠ¤íŠ¸ë¦¬ë° ëª¨ë¸: ReadableStreamìœ¼ë¡œ ê°ì‹¸ì„œ ì¦‰ì‹œ ë°˜í™˜ (Netlify ì•ˆì „ íŒ¨í„´)
    if (isStreamableGPT && OPENAI_STREAMING_ALLOWED) {
      console.log('[Chat API] Using streaming path for model:', modelId);
      const stream = new ReadableStream({
        async start(ctrl) {
          console.log('[Chat API Stream] Stream started, calling OpenAI...');
          try {
            console.log('[Chat API Stream] About to call callOpenAI with streaming=true');
            const openaiRes = await callOpenAI(modelId, messages, userAttachments, persona, languageInstructionWithMemory, true) as Response;
            console.log('[Chat API Stream] callOpenAI returned, status:', openaiRes.status);
            
            if (!openaiRes.body) {
              throw new Error('OpenAI response has no body');
            }
            
            const reader = openaiRes.body.getReader();
            console.log('[Chat API Stream] Reading stream...');
            try {
              let chunkCount = 0;
              while (true) {
                const { done, value } = await reader.read();
                if (done) {
                  console.log('[Chat API Stream] Stream complete, chunks received:', chunkCount);
                  break;
                }
                chunkCount++;
                ctrl.enqueue(value);
              }
            } finally {
              reader.releaseLock();
            }
            ctrl.close();
          } catch (err: any) {
            // ìŠ¤íŠ¸ë¦¼ ë‚´ë¶€ ì—ëŸ¬ë¥¼ SSE í˜•ì‹ìœ¼ë¡œ ì „ë‹¬
            console.error('[Chat API Stream] Error in stream:', {
              name: err.name,
              message: err.message,
              stack: err.stack?.split('\n').slice(0, 3).join('\n')
            });
            const errMsg = err.message || 'ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.';
            const errEvent = `data: ${JSON.stringify({ error: errMsg })}\n\ndata: [DONE]\n\n`;
            ctrl.enqueue(new TextEncoder().encode(errEvent));
            ctrl.close();
          }
        }
      });

      return new Response(stream, {
        headers: {
          'Content-Type': 'text/event-stream',
          'Cache-Control': 'no-cache',
          'Connection': 'keep-alive',
        },
      });
    }

    // ë‚˜ë¨¸ì§€ ëª¨ë¸ì€ JSON ì‘ë‹µ
    let responseContent: string;

    if (isOpenAIModel) {
      console.log(`[Chat API] Calling OpenAI API for model: ${modelId}${modelId === 'gptimage1' || modelId === 'dalle3' ? ' (Image Generation)' : ''}`);
      responseContent = await callOpenAI(modelId, messages, userAttachments, persona, languageInstructionWithMemory) as string;
      console.log('[Chat API] OpenAI response received, length:', responseContent?.length || 0);
    } else if (modelId.startsWith('gemini')) {
      if (process.env.NODE_ENV !== 'production') {
        console.log('[Chat API] Calling Gemini API');
      }
      responseContent = await callGemini(modelId, applyLanguageInstruction(messages), userAttachments, 0, temperature);
    } else if (modelId.startsWith('claude') || modelId === 'haiku35' || modelId === 'haiku45' || modelId === 'sonnet45' || modelId === 'opus4' || modelId === 'opus41' || modelId === 'opus45' || modelId === 'opus46') {
      if (process.env.NODE_ENV !== 'production') {
        console.log('[Chat API] Calling Anthropic API');
      }
      responseContent = await callAnthropic(modelId, applyLanguageInstruction(messages), userAttachments, 0, temperature);
    } else if (modelId.startsWith('perplexity') || modelId === 'sonar' || modelId === 'sonarPro' || modelId === 'deepResearch') {
      if (process.env.NODE_ENV !== 'production') {
        console.log('[Chat API] Calling Perplexity API');
      }
      responseContent = await callPerplexity(modelId, applyLanguageInstruction(messages), userAttachments, 0, temperature);
    } else {
      if (process.env.NODE_ENV !== 'production') {
        console.log('[Chat API] Unknown model, using demo response');
      }
      responseContent = `[${modelId}] ${resolvedLanguage === 'ja' ? 'ã“ã‚“ã«ã¡ã¯ï¼è³ªå•ã«ãŠç­”ãˆã—ã¾ã™ã€‚' : resolvedLanguage === 'en' ? 'Hello! I will answer your question.' : 'ì•ˆë…•í•˜ì„¸ìš”! ì§ˆë¬¸ì— ë‹µë³€ë“œë¦¬ê² ìŠµë‹ˆë‹¤.'} (API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•„ ë°ëª¨ ëª¨ë“œë¡œ ì‹¤í–‰ ì¤‘ì…ë‹ˆë‹¤. .env.local íŒŒì¼ì— API í‚¤ë¥¼ ì¶”ê°€í•˜ì„¸ìš”.)`;
    }

    return NextResponse.json({ content: responseContent });

  } catch (error: any) {
    // ëª¨ë“  í™˜ê²½ì—ì„œ ì—ëŸ¬ ë¡œê¹… (ë””ë²„ê¹…ìš©)
    console.error('[Chat API] Error caught:', {
      message: error.message,
      name: error.name,
      stack: error.stack?.split('\n').slice(0, 3).join('\n'), // ìŠ¤íƒ ì¼ë¶€ë§Œ
      timestamp: new Date().toISOString()
    });
    
    // API í‚¤ê°€ ì—†ëŠ” ê²½ìš° ë°ëª¨ ì‘ë‹µ
    if (error.message?.includes('API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤')) {
      return NextResponse.json({ 
        content: `ğŸ’¡ ë°ëª¨ ëª¨ë“œ: ì‹¤ì œ AI ì‘ë‹µì„ ë°›ìœ¼ë ¤ë©´ .env.local íŒŒì¼ì— API í‚¤ë¥¼ ì¶”ê°€í•˜ì„¸ìš”.\n\n` +
                 `ì„¤ì • ë°©ë²•:\n` +
                 `1. í”„ë¡œì íŠ¸ ë£¨íŠ¸ì— .env.local íŒŒì¼ ìƒì„±\n` +
                 `2. ë‹¤ìŒ í™˜ê²½ë³€ìˆ˜ ì¶”ê°€:\n` +
                 `   - OPENAI_API_KEY=your_key (GPT ëª¨ë¸ìš©)\n` +
                 `   - ANTHROPIC_API_KEY=your_key (Claude ëª¨ë¸ìš©)\n` +
                 `   - PERPLEXITY_API_KEY=your_key (Perplexity ëª¨ë¸ìš©)\n\n` +
                 `ìì„¸í•œ ë‚´ìš©ì€ env.example íŒŒì¼ì„ ì°¸ê³ í•˜ì„¸ìš”.`
      });
    }

    // ëª¨ë¸ ì‘ë‹µ íƒ€ì„ì•„ì›ƒ ì²˜ë¦¬
    if (error.message === 'MODEL_RESPONSE_TIMEOUT' || error.name === 'AbortError') {
      console.error('[Chat API] Timeout error - Netlify function timeout likely exceeded');
      return NextResponse.json(
        { error: `AI ëª¨ë¸ ì‘ë‹µ ì‹œê°„ì´ ì´ˆê³¼ë˜ì—ˆìŠµë‹ˆë‹¤ (ì œí•œ: ${Math.round(DEFAULT_API_TIMEOUT_MS / 1000)}ì´ˆ). ë” ì§§ì€ ì§ˆë¬¸ìœ¼ë¡œ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.` },
        { status: 504 }
      );
    }

    // ì—ëŸ¬ íƒ€ì…ë³„ ì²˜ë¦¬
    let errorMessage = 'ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.';
    let statusCode = 500;

    if (error.message?.includes('API í‚¤')) {
      errorMessage = error.message;
      statusCode = 401;
    } else if (error.message?.includes('í•œë„')) {
      errorMessage = error.message;
      statusCode = 429;
    } else if (error.message?.includes('ë„¤íŠ¸ì›Œí¬') || error.message?.includes('fetch')) {
      errorMessage = 'ë„¤íŠ¸ì›Œí¬ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ì—°ê²°ì„ í™•ì¸í•˜ì„¸ìš”.';
      statusCode = 503;
    } else if (error.message) {
      errorMessage = error.message;
    }
    
    return NextResponse.json(
      { error: errorMessage },
      { status: statusCode }
    );
  }
}

